
@misc{ouyang_kernelbench_2025,
	title = {{KernelBench}: Can {LLMs} Write Efficient {GPU} Kernels?},
	url = {http://arxiv.org/abs/2502.10517},
	doi = {10.48550/arXiv.2502.10517},
	shorttitle = {{KernelBench}},
	abstract = {Efficient {GPU} kernels are crucial for building performant machine learning architectures, but writing them is a time-consuming challenge that requires significant expertise; therefore, we explore using language models ({LMs}) to automate kernel generation. We introduce {KernelBench}, an open-source framework for evaluating {LMs}' ability to write fast and correct kernels on a suite of 250 carefully selected {PyTorch} {ML} workloads. {KernelBench} represents a real-world engineering environment and making progress on the introduced benchmark directly translates to faster practical kernels. We introduce a new evaluation metric fast\_p, which measures the percentage of generated kernels that are functionally correct and offer a speedup greater than an adjustable threshold p over baseline. Our experiments across various state-of-the-art models and test-time methods show that frontier reasoning models perform the best out of the box but still fall short overall, matching the {PyTorch} baseline in less than 20\% of the cases. While we show that results can improve by leveraging execution and profiling feedback during iterative refinement, {KernelBench} remains a challenging benchmark, with its difficulty increasing as we raise speedup threshold p.},
	number = {{arXiv}:2502.10517},
	publisher = {{arXiv}},
	author = {Ouyang, Anne and Guo, Simon and Arora, Simran and Zhang, Alex L. and Hu, William and RÃ©, Christopher and Mirhoseini, Azalia},
	urldate = {2025-10-03},
	date = {2025-02-14},
	eprinttype = {arxiv},
	eprint = {2502.10517 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, Computer Science - Software Engineering},
	file = {Preprint PDF:/home/arjun/Zotero/storage/8GUR2R2X/Ouyang et al. - 2025 - KernelBench Can LLMs Write Efficient GPU Kernels.pdf:application/pdf;Snapshot:/home/arjun/Zotero/storage/CPWK6RR7/2502.html:text/html},
}

@misc{chen_cuda-llm_2025,
	title = {{CUDA}-{LLM}: {LLMs} Can Write Efficient {CUDA} Kernels},
	url = {http://arxiv.org/abs/2506.09092},
	doi = {10.48550/arXiv.2506.09092},
	shorttitle = {{CUDA}-{LLM}},
	abstract = {Large Language Models ({LLMs}) have demonstrated strong capabilities in general-purpose code generation. However, generating the code which is deeply hardware-specific, architecture-aware, and performance-critical, especially for massively parallel {GPUs}, remains a complex challenge. In this work, we explore the use of {LLMs} for the automated generation and optimization of {CUDA} programs, with the goal of producing high-performance {GPU} kernels that fully exploit the underlying hardware. To address this challenge, we propose a novel framework called {\textbackslash}textbf\{Feature Search and Reinforcement ({FSR})\}. {FSR} jointly optimizes compilation and functional correctness, as well as the runtime performance, which are validated through extensive and diverse test cases, and measured by actual kernel execution latency on the target {GPU}, respectively. This approach enables {LLMs} not only to generate syntactically and semantically correct {CUDA} code but also to iteratively refine it for efficiency, tailored to the characteristics of the {GPU} architecture. We evaluate {FSR} on representative {CUDA} kernels, covering {AI} workloads and computational intensive algorithms. Our results show that {LLMs} augmented with {FSR} consistently guarantee correctness rates. Meanwhile, the automatically generated kernels can outperform general human-written code by a factor of up to 179\${\textbackslash}times\$ in execution speeds. These findings highlight the potential of combining {LLMs} with performance reinforcement to automate {GPU} programming for hardware-specific, architecture-sensitive, and performance-critical applications.},
	number = {{arXiv}:2506.09092},
	publisher = {{arXiv}},
	author = {Chen, Wentao and Zhu, Jiace and Fan, Qi and Ma, Yehan and Zou, An},
	urldate = {2025-10-03},
	date = {2025-06-10},
	eprinttype = {arxiv},
	eprint = {2506.09092 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/arjun/Zotero/storage/WNRPG83M/Chen et al. - 2025 - CUDA-LLM LLMs Can Write Efficient CUDA Kernels.pdf:application/pdf;Snapshot:/home/arjun/Zotero/storage/2EARGVYR/2506.html:text/html},
}

@misc{zhao_shard_2025,
	title = {{SHARD}: Securing {GPU} Kernels with Lightweight Formal Methods},
	url = {https://csslab-ustc.github.io/publications/2025/gpu-security-full.pdf},
	author = {Zhao, Jiacheng and Hua, Baojian},
	urldate = {2025-10-02},
	date = {2025},
}

@inproceedings{betts_gpuverify_2012,
	location = {Tucson Arizona {USA}},
	title = {{GPUVerify}: a verifier for {GPU} kernels},
	isbn = {978-1-4503-1561-6},
	url = {https://dl.acm.org/doi/10.1145/2384616.2384625},
	doi = {10.1145/2384616.2384625},
	shorttitle = {{GPUVerify}},
	eventtitle = {{SPLASH} '12: Conference on Systems, Programming, and Applications: Software for Humanity},
	pages = {113--132},
	booktitle = {Proceedings of the {ACM} international conference on Object oriented programming systems languages and applications},
	publisher = {{ACM}},
	author = {Betts, Adam and Chong, Nathan and Donaldson, Alastair and Qadeer, Shaz and Thomson, Paul},
	urldate = {2025-10-03},
	date = {2012-10-19},
	langid = {english},
}

@misc{the_dafny-lang_community_dafny_2025,
	title = {Dafny Documentation},
	url = {https://dafny.org/dafny/DafnyRef/DafnyRef},
	author = {The dafny-lang community},
	date = {2025-09-26},
}

@misc{solar-lezama_introduction_2025,
	title = {Introduction to Program Synthesis},
	url = {https://people.csail.mit.edu/asolar/SynthesisCourse/Lecture1.htm},
	author = {Solar-Lezama, Armando},
	urldate = {2025-10-02},
	date = {2025},
}